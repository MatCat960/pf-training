# -*- coding: utf-8 -*-
"""16_unsupervised_coverage.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L4D-Lr36v55xlxgYpCa0ly95aGLpXLnW

# Training a Neural Network to perform distributed Coverage **with unknown number of robots**
"""

import torch
from torch import nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np
import math

from pathlib import Path
from copy import deepcopy as dc

# Setup device agnostic code
device = 'cuda' if torch.cuda.is_available() else 'cpu'

print(f"Using device: {device}")

from google.colab import drive
drive.mount('/content/drive')

path = Path("/content/drive/MyDrive/Colab Notebooks/PyTorch tutorial/dynamic_coverage_vel3").glob('**/*')
# path = Path("/content/drive/MyDrive/Colab Notebooks/PyTorch tutorial/dynamic_coverage_vel/log_6_robots.txt")
files = [x for x in path if x.is_file()]

len(files)

ROBOTS_NUM = 20
lookback = 7
AREA_W = 30.0
AREA_H = 30.0
ROBOT_RANGE = 5.0
ROBOT_FOV = 120.0

"""## Utility Functions"""

def in_fov(robot, target, fov, range):
  fov_rad = fov * math.pi / 180.0
  xr = robot[0]
  yr = robot[1]
  phi = robot[2]
  dx = target[0] - xr
  dy = target[1] - yr
  dist = math.sqrt(dx**2 + dy**2)
  if dist > range:
    return 0

  xrel = dx * math.cos(phi) + dy * math.sin(phi)
  yrel = -dy * math.sin(phi) + dy * math.cos(phi)
  angle = abs(math.atan2(yrel, xrel))
  if (angle <= fov_rad) and (xrel >= 0.0):
    return 1
  else:
    return 0

def gauss_pdf(x, y, means, covs):
  """
  Calculate the probability in the cell (x,y)

  Args:
    x (float) : x coord of the considered point
    y (float) : y coord of the considered point
    means (list(np.array)) : list of mean points
    covs (list(np.array)) : list of covariance matrices
  """

  prob = 0.0
  for i in range(len(means)):
    m = means[i]
    cov = covs[i]
    exp = -0.5 * np.sum

# X, Y : meshgrid
def multigauss_pdf(X, Y, means, sigmas):
  # Flatten the meshgrid coordinates
  points = np.column_stack([X.flatten(), Y.flatten()])

  # Number of components in the mixture model
  num_components = len(means)


  # Initialize the probabilities
  probabilities = np.zeros_like(X)

  # Calculate the probability for each component
  for i in range(num_components):
      mean = means[i]
      covariance = sigmas[i]

      # Calculate the multivariate Gaussian probability
      exponent = -0.5 * np.sum((points - mean) @ np.linalg.inv(covariance) * (points - mean), axis=1)
      coefficient = 1 / np.sqrt((2 * np.pi) ** 2 * np.linalg.det(covariance))
      component_prob = coefficient * np.exp(exponent)

      # Add the component probability weighted by its weight
      probabilities += component_prob.reshape(X.shape)

  return probabilities

def plot_fov(fov_deg, radius, ax):
  # fig = plt.figure(figsize=(6,6))
  # plt.scatter(neighs[:, 0], neighs[:, 1], marker='*')

  x1 = np.array([0.0, 0.0, 0.0])
  fov = fov_deg * math.pi / 180
  arc_theta = np.arange(-0.5*fov, 0.5*fov, 0.01*math.pi)
  th = np.arange(fov/2, 2*math.pi+fov/2, 0.01*math.pi)

  # FOV
  xfov = radius * np.cos(arc_theta)
  xfov = np.append(x1[0], xfov)
  xfov = np.append(xfov, x1[0])
  yfov = radius * np.sin(arc_theta)
  yfov = np.append(x1[1], yfov)
  yfov = np.append(yfov, x1[1])
  ax.plot(xfov, yfov)

from numpy import linalg as LA

def plot_ellipse(ctr, cov, ax, s=4.605):
  """
  Args:
    ctr (np.array(1, 2)): center of the ellipse
    cov (np.array(2, 2)): covariance matrix
    s (double): confidence interval
  """

  epsilon = 0.01

  eigenvalues, eigenvectors = LA.eigh(cov)
  eigenvalues = eigenvalues + epsilon
  # print(f"Eigenvalues: {eigenvalues}")
  # if eigenvalues[0] < 0.0 or eigenvalues[1] < 0.0:
  #   print(f"Cov matrix: {cov}")
  #   print(f"Eigenvalues: {eigenvalues}")
  # eigenvalues[eigenvalues < 0.0] = 0.1
  a = math.sqrt(s*abs(eigenvalues[0]))
  b = math.sqrt(s*abs(eigenvalues[1]))

  if (a < b):
    temp = dc(a)
    a = dc(b)
    b = temp

  print(f"Major axis: {a}")

  m = 0
  l = 1
  if eigenvalues[1] > eigenvalues[0]:
    m = 1
    l = 0

  theta = math.atan2(eigenvectors[1,m], eigenvectors[0,m])
  if theta < 0.0:
    theta += math.pi

  vx = []; vy = []
  x = ctr[0]
  y = ctr[1]
  for phi in np.arange(0, 2*np.pi, 0.1):
    xs = x + a * np.cos(phi) * np.cos(theta) - b * np.sin(phi) * np.sin(theta)
    ys = y + a * np.cos(phi) * np.sin(theta) + b * np.sin(phi) * np.cos(theta)
    vx.append(xs)
    vy.append(ys)

  vx.append(vx[0])
  vy.append(vy[0])

  # fig = plt.figure(figsize=(6,6))
  ax.plot(vx, vy)

def mirror(points):
    mirrored_points = []

    # Define the corners of the square
    square_corners = [(-0.5*AREA_W, -0.5*AREA_W), (0.5*AREA_W, -0.5*AREA_W), (0.5*AREA_W, 0.5*AREA_W), (-0.5*AREA_W, 0.5*AREA_W)]

    # Mirror points across each edge of the square
    for edge_start, edge_end in zip(square_corners, square_corners[1:] + [square_corners[0]]):
        edge_vector = (edge_end[0] - edge_start[0], edge_end[1] - edge_start[1])

        for point in points:
            # Calculate the vector from the edge start to the point
            point_vector = (point[0] - edge_start[0], point[1] - edge_start[1])

            # Calculate the mirrored point by reflecting across the edge
            mirrored_vector = (point_vector[0] - 2 * (point_vector[0] * edge_vector[0] + point_vector[1] * edge_vector[1]) / (edge_vector[0]**2 + edge_vector[1]**2) * edge_vector[0],
                               point_vector[1] - 2 * (point_vector[0] * edge_vector[0] + point_vector[1] * edge_vector[1]) / (edge_vector[0]**2 + edge_vector[1]**2) * edge_vector[1])

            # Translate the mirrored vector back to the absolute coordinates
            mirrored_point = (edge_start[0] + mirrored_vector[0], edge_start[1] + mirrored_vector[1])

            # Add the mirrored point to the result list
            mirrored_points.append(mirrored_point)

    return mirrored_points

data = []
sizes = []
# for file in files:
with open(files[0]) as f:
  lines = f.readlines()
  sizes.append(len(lines))

for l in lines:
  data.append(l)

print(data[0])

poses = np.zeros([len(data), 2], dtype="float32")

for i in range(len(data)):
  data[i] = data[i].replace('\n', '')
  poses[i] = tuple(map(float, data[i].split(' ')))

len(data)/ROBOTS_NUM

"""## Split poses and velocities"""

pos = np.zeros((int(len(data)/2), 2), dtype="float32")
vel = np.zeros_like(pos)

print(poses[0])
print(f"Original length: {len(poses)}")
print(f"Pos length: {len(pos)}")

for i in range(0, len(pos)):
  pos[i] = poses[2*i]
  vel[i] = poses[2*i+1]

pos[pos == 100.0] = 0.0
vel[vel == 99.9] = 0.0

pos[0]

"""## Convert numpy to torch.Tensor"""

X = torch.from_numpy(pos).to(device)
y = torch.from_numpy(vel).to(device)

print(X[:ROBOTS_NUM, :])
X.shape, y.shape



# X, y = X[:-4], y[:-4]
s = int(X.shape[0]/ROBOTS_NUM)

print(f"Original shape: {X.shape}")
X = X[:s*ROBOTS_NUM]
y = y[:s*ROBOTS_NUM]
print(f"Final shape: {X.shape}")

X[:ROBOTS_NUM, :]

X = X.view(-1, 2*ROBOTS_NUM)
y = y.view(-1, 2*ROBOTS_NUM)

X.shape, y.shape

import random
rnd = random.randint(0, int(X.shape[0]/ROBOTS_NUM))
print(rnd)
X[rnd, :]

"""## Scale values"""

from sklearn.preprocessing import MinMaxScaler

# scaler = MinMaxScaler(feature_range=(-1,1))
# X_scaled = scaler.fit_transform(X.cpu())
# # y_scaled = scaler.fit_transform(y.cpu())
# X_new = np.float32(X_scaled)
# # y_new = y_scaled

# X = torch.from_numpy(X_new)
# print(type(X))
# y = torch.from_numpy(y_new)

"""## Create train and test split"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,
                                                   y,
                                                   test_size=0.2)

y_train = y_train.squeeze(1)
y_test = y_test.squeeze(1)
X_train.shape, X_test.shape, y_train.shape, y_test.shape

from torch.utils.data import TensorDataset, DataLoader

# create TensorDatasets for training and testing sets
train_dataset = TensorDataset(X_train, y_train)
test_dataset = TensorDataset(X_test, y_test)

# create DataLoaders
batch_size = 64
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

for input, target in train_loader:
  print(input.shape, target.shape)
  break

"""## Define neural network model"""

class CoverageModel(nn.Module):
  def __init__(self, input_size, output_size):
    super().__init__()
    self.input_size = input_size
    self.output_size = output_size

    self.fc1 = nn.Linear(input_size, 128)
    self.fc2 = nn.Linear(128, 64)
    self.fc3 = nn.Linear(64, output_size)
    self.relu = nn.ReLU(inplace=True)
    # self.activation = nn.Sigmoid()
    self.activation = nn.Tanh()

  def forward(self, x):
    x = self.activation(self.fc1(x))
    x = self.activation(self.fc2(x))
    x = self.fc3(x)

    return x

class DynamicCoverageModel(nn.Module):
  def __init__(self, max_size):
    super().__init__()
    self.max_size = max_size
    self.input_size = max_size
    self.output_size = max_size

    self.fc1 = nn.Linear(self.input_size, 64)
    self.fc2 = nn.Linear(64, 32)
    self.fc3 = nn.Linear(32, self.output_size)
    self.relu = nn.ReLU()
    self.activation = nn.Tanh()

  def reshape(self, x):
    (x == 100.0).nonzero(as_tuple=True)

    size = x.shape[1]
    self.input_size = size
    self.output_size = size

    self.fc1 = nn.Linear(self.input_size, 64)
    self.fc3 = nn.Linear(32, self.output_size)

  def forward(self, x):
    self.reshape(x)
    x = self.activation(self.fc1(x))
    x = self.activation(self.fc2(x))
    x = self.fc3(x)

    return x

class CoverageModel2(nn.Module):
  def __init__(self, input_size, output_size):
    super().__init__()
    self.input_size = input_size
    self.output_size = output_size

    self.fc1 = nn.Linear(input_size, 256)
    self.fc2 = nn.Linear(256, 128)
    self.fc3 = nn.Linear(128, 64)
    self.fc4 = nn.Linear(64, 32)
    self.fc5 = nn.Linear(32, output_size)
    self.relu = nn.ReLU()
    # self.activation = nn.Sigmoid()
    self.activation = nn.Tanh()


  def forward(self, x):

    in_size = 0
    # print(x.shape)
    for i in range(x.shape[1]):
      # print(row.shape)
      if x[0, i] != 0.0:
        in_size += 1

    # print("x:")
    # print(x)
    # print("in_size: {}".format(in_size))

    x = self.activation(self.fc1(x))
    x = self.activation(self.fc2(x))
    x = self.activation(self.fc3(x))
    x = self.activation(self.fc4(x))
    x = self.fc5(x)

    out = np.zeros((x.shape[0], self.input_size), dtype="float32")
    out = torch.from_numpy(out).to(device)
    out[:, :in_size] = x[:, :in_size]

    # out = out.to(device)

    return out

class DropoutCoverageModel(nn.Module):
  def __init__(self, input_size, output_size):
    super().__init__()
    self.input_size = input_size
    self.output_size = output_size

    self.fc1 = nn.Linear(input_size, 128)
    self.dropout1 = nn.Dropout(0.2)
    self.fc2 = nn.Linear(128, 128)
    self.dropout2 = nn.Dropout(0.2)
    self.fc3 = nn.Linear(128, output_size)
    self.relu = nn.ReLU()
    # self.activation = nn.Sigmoid()
    self.activation1 = nn.Tanh()
    self.activation2 = nn.Tanh()


  def forward(self, x):

    in_size = 0
    # print(x.shape)
    for i in range(x.shape[1]):
      # print(row.shape)
      if x[0, i] != 0.0:
        in_size += 1

    # print("x:")
    # print(x)
    # print("in_size: {}".format(in_size))

    x = self.dropout1(self.activation1(self.fc1(x)))
    x = self.dropout2(self.activation2(self.fc2(x)))
    # x = self.activation(self.fc3(x))
    # x = self.activation(self.fc4(x))
    x = self.fc3(x)

    out = np.zeros((x.shape[0], self.input_size), dtype="float32")
    out = torch.from_numpy(out).to(device)
    out[:, :in_size] = x[:, :in_size]

    # out = out.to(device)

    return out

"""## Training"""

from shapely import Polygon, Point, intersection
from scipy.spatial import Voronoi, voronoi_plot_2d

def myLoss(x_pred, r_sens = 5.0, dt = 0.2):
  """
  Calculate the Loss function as the distance to optimal configuration, which maximizes the covered area.
  Assumes a uniform probability distribution.
  """

  # Calculate the number of robots
  n_robots = 0
  # print(f"x_pred shape: {x_pred.shape}")
  xt = x_pred[0]
  for i in range(xt.shape[0]):
    if xt[i, 0] != 0.0:
      n_robots += 1

  print(f"Number of robots: {n_robots}")
  # n_robots = int(n_robots/2)


  # Calculate maximum area covered by n_robots
  At = n_robots * np.pi * r_sens**2
  dummy_points = torch.zeros((x_pred.shape[0], 5*n_robots, 2))
  dummy_points[:, :n_robots, :] = x_pred[:, :n_robots, :]

  loss = 0.0
  for i in range(x_pred.shape[0]):
    xi = x_pred[i, :n_robots, :]
    mirrored_points = mirror(xi)
    mir_pts = torch.Tensor(mirrored_points)
    dummy_points[i, n_robots:, :] = mir_pts

    vor = Voronoi(dummy_points[i, :, :].cpu().detach().numpy())

    for idx in range(n_robots):
      region = vor.point_region[idx]
      poly_vert = []

      for vert in vor.regions[region]:
        v = vor.vertices[vert]
        poly_vert.append(v)

      poly = Polygon(poly_vert)
      robot = xi[idx, :]
      step = 0.5
      range_pts = []
      for th in np.arange(0.0, 2*np.pi, step):
        xp = robot[0] + r_sens * np.cos(th)
        yp = robot[1] + r_sens * np.sin(th)
        pt = Point(xp.cpu().detach().numpy(), yp.cpu().detach().numpy())
        range_pts.append(pt)

      range_poly = Polygon(range_pts)
      lim_region = intersection(poly, range_poly)

      centr = np.array([lim_region.centroid.x, lim_region.centroid.y])
      dist = np.linalg.norm(robot.cpu().detach().numpy()-centr)

      loss += dist


  return loss

model = DropoutCoverageModel(2 * ROBOTS_NUM, 2 * ROBOTS_NUM)
model = model.to(device)
loss_fn = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

RUN_BATCHED = False

X_train[0, :]

"""## Train on unbatched data"""

if not RUN_BATCHED:
  epochs = 100
  epsilon = 0.01
  xl = X_train.view(-1, ROBOTS_NUM, 2)
  dt = 0.2

  for epoch in range(epochs):
    model.train()
    y_pred = model(X_train)
    if epoch == 0:
      print(f"output shape: {y_pred.shape}")
      print(f"Target shape: {y_train.shape}")
    mse_loss = loss_fn(y_pred, y_train)

    ### Calculate loss related to distance to centroid
    vel = y_pred.view(-1, ROBOTS_NUM, 2)
    xn = xl + vel * dt
    dist_loss = myLoss(xn)
    print(f"Dist Loss: {dist_loss}")

    loss = mse_loss + 0.01*dist_loss
    torch.autograd.set_detect_anomaly(True)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    ### Testing
    # model.eval()
    # with torch.inference_mode():
    #   test_pred = model(X_test)
    #   test_loss = loss_fn(test_pred, y_test)

    print(f"Epoch: {epoch} | Loss: {loss.item()}")

# for name, param in model.named_parameters():
#   if param.requires_grad:
#     print(name, param)

"""## Test on simulated robots"""

"""
import random
N_ROBOTS = 16
robots = np.zeros((N_ROBOTS, 2), dtype="float32")
for i in range(N_ROBOTS):
  robots[i, :] = -20.0 + 40.0 * np.random.rand(1, 2)

robots_dummy = np.zeros((ROBOTS_NUM, 2), dtype="float32")
robots_dummy[:N_ROBOTS, :] = robots
# robots = np.array(([-4.0, 4.0],
#                   [-4.0, -4.0],
#                   [4.0, -4.0],
#                   [4.0, 4.0],
#                   [6.0, 0.0],
#                   [-6.0, 0.0]),
#                   dtype="float32")

# robots = robots - 8.0
plt.scatter(robots[:, 0], robots[:, 1])
Xt = torch.from_numpy(robots_dummy)
Xt = Xt.view(-1, ROBOTS_NUM*2)
Xt = Xt.to(device)

robots_dummy[:ROBOTS_NUM, :]

Xt[0, 0]



NUM_STEPS = 500
dt = 0.2

X_hist = [Xt]
# v_hist = []

r_hist = []

for i in range(ROBOTS_NUM):
  r = []
  r_hist.append(r)

robots_hist = torch.Tensor(NUM_STEPS, ROBOTS_NUM, 2)
print(robots_hist.shape)

for i in range(NUM_STEPS):
  # get velocity
  v_pred = model(Xt)
  if i % 100 == 0.0:
    print(f"Vpred : {v_pred}")

  # move robots
  # v = v_pred.view(ROBOTS_NUM, 2)

  # for j in range(2*ROBOTS_NUM):
  Xt[0, :] = Xt[0, :] + v_pred[0, :] * dt
  # print(f"Actual Xt: {Xt}")

  xp = Xt.view(ROBOTS_NUM, 2)
  for j in range(ROBOTS_NUM):
    robots_hist[i, j, :] = xp[j, :]

  X_hist.append(Xt)

robots_hist[:, 0, :]

for i in range(N_ROBOTS):
  plt.plot(robots_hist[:, i, 0].cpu().detach().numpy(), robots_hist[:, i, 1].cpu().detach().numpy())

  # for i in range(ROBOTS_NUM):
  plt.scatter(robots_hist[-1, i, 0].cpu().detach().numpy(), robots_hist[-1, i, 1].cpu().detach().numpy())

plt.plot(0.0, 0.0, '*')


for i in range(N_ROBOTS):
  plt.scatter(robots_hist[-1, i, 0].cpu().detach().numpy(), robots_hist[-1, i, 1].cpu().detach().numpy())

plt.plot(0.0, 0.0, '*')
"""